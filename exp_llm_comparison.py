# ==============================================================================
# ğŸ† UNIFIED LLM BENCHMARK SUITE: Zero-Shot & 3-Shot
# Files: exp_llm_comparison.py
# Models: Qwen/Qwen2.5-7B-Instruct, unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit
# Logic: Unified validation set + Standard Chat Template + Regular Expression Parsing
# ==============================================================================

import os
import gc
import re
import torch
import random
import pandas as pd
import numpy as np
from tqdm import tqdm
from sklearn.metrics import f1_score, accuracy_score
from sklearn.model_selection import train_test_split
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# æŠ‘åˆ¶ HuggingFace çš„ä¸€äº›å†—ä½™è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")

# =========================== âš™ï¸ å…¨å±€é…ç½® ===========================
SEED = 42
BATCH_SIZE = 1

# å®šä¹‰å®éªŒä»»åŠ¡åˆ—è¡¨
# ç»“æ„: (Model_Path, Model_Short_Name)
MODELS_TO_TEST = [
    ("Qwen/Qwen2.5-7B-Instruct", "Qwen-2.5-7B"),
    ("unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit", "Llama-3.1-8B")
]

DATASETS = ["SMP2020", "SST-5", "TweetEval"]

# =========================== ğŸ› ï¸ å·¥å…·å‡½æ•° ===========================

def set_seed(seed=SEED):
    """å›ºå®šæ‰€æœ‰éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def get_validation_set(dataset_name):
    """
    ç»Ÿä¸€éªŒè¯é›†åŠ è½½é€»è¾‘ (ç»å¯¹å…¬å¹³)
    ä¿æŒäº†åŸå§‹ä»£ç ä¸­çš„æ•°æ®å¤„ç†å’Œé‡‡æ ·é€»è¾‘
    """
    set_seed(SEED) # ç¡®ä¿æ¯æ¬¡é‡‡æ ·ä¸€è‡´
    
    if dataset_name == "SMP2020":
        # print(f"ğŸ“š Loading {dataset_name}...")
        ds = load_dataset("Um1neko/smp2020", split="train")
        df = pd.DataFrame(ds)
        if "content" in df.columns: df = df.rename(columns={"content": "text"})
        df = df.dropna(subset=["text", "label"])
        df["label"] = df["label"].astype(int)
        val_count = 80 
        
    elif dataset_name == "SST-5":
        # print(f"ğŸ“š Loading {dataset_name}...")
        ds = load_dataset("SetFit/sst5", split="train")
        df = pd.DataFrame(ds)
        if "sentence" in df.columns: df = df.rename(columns={"sentence": "text"})
        if "label_text" in df.columns: df = df.drop(columns=["label_text"])
        df = df[["text", "label"]].dropna()
        df["label"] = df["label"].astype(int)
        val_count = 100
        
    elif dataset_name == "TweetEval":
        # print(f"ğŸ“š Loading {dataset_name}...")
        ds = load_dataset("tweet_eval", "sentiment", split="train")
        df = pd.DataFrame(ds)
        df = df[["text", "label"]].dropna()
        df["label"] = df["label"].astype(int)
        val_count = 100
    
    # Stratified Split & Sampling
    try:
        _, val_pool = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=SEED)
    except ValueError:
        # Fallback if dataset is too small for stratification
        val_pool = df.sample(frac=0.2, random_state=SEED)

    num_labels = df['label'].nunique()
    sampled_dfs = []
    
    for label in range(num_labels):
        class_df = val_pool[val_pool['label'] == label]
        if len(class_df) > 0:
            n_samples = min(len(class_df), val_count)
            sampled_dfs.append(class_df.sample(n=n_samples, random_state=SEED))
    
    final_df = pd.concat(sampled_dfs).sample(frac=1, random_state=SEED).reset_index(drop=True)
    # print(f"âœ… Validation Set ({dataset_name}): {len(final_df)} samples.")
    return final_df

def get_prompt_content(dataset_name, text, model_short_name, shot_mode):
    """
    ç»Ÿä¸€ Prompt æ„é€ å·¥å‚
    æ ¹æ® æ¨¡å‹(Qwen/Llama) å’Œ æ¨¡å¼(Zero/3-Shot) è¿”å›å¯¹åº”çš„ Prompt
    """
    # ------------------- 3-Shot Prompts (Unified) -------------------
    # æ ¹æ®æä¾›çš„ä»£ç ï¼Œ3-Shot çš„ Prompt åœ¨ Qwen å’Œ Llama ä¸Šä½¿ç”¨äº†ç›¸åŒçš„æ¨¡æ¿
    if shot_mode == "3-shot":
        if dataset_name == "SMP2020":
            return f"""ä»»åŠ¡ï¼šåˆ¤æ–­æ–‡æœ¬çš„æƒ…æ„Ÿç±»åˆ«ã€‚
å‚è€ƒç¤ºä¾‹ï¼š
æ–‡æœ¬: "ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œå¿ƒæƒ…å¥½æäº†ï¼" -> ç±»åˆ«: 2
æ–‡æœ¬: "è¿™æœåŠ¡æ€åº¦å¤ªå·®äº†ï¼Œæ°”æ­»æˆ‘äº†ã€‚" -> ç±»åˆ«: 0
æ–‡æœ¬: "è¿™å°±æ˜¯ä¸€æœ¬æ™®é€šçš„ä¹¦ã€‚" -> ç±»åˆ«: 3

è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬åˆ†ç±»ï¼š
æ–‡æœ¬: "{text}"
ç±»åˆ«é€‰é¡¹: 0:æ„¤æ€’, 1:ææƒ§, 2:é«˜å…´, 3:ä¸­æ€§, 4:æ‚²ä¼¤, 5:æƒŠå¥‡
è¯·ä»…è¾“å‡ºä¸€ä¸ªæ•°å­— ID (0-5)ã€‚
ç­”æ¡ˆ:"""
        elif dataset_name == "SST-5":
            return f"""Task: Classify the sentiment.
Examples:
Text: "An absolute masterpiece, thrilling from start to finish." -> Class: 4
Text: "Boring, predictable, and a waste of time." -> Class: 0
Text: "It's a movie that exists." -> Class: 2

Classify this:
Text: "{text}"
Options: 0:Very Negative, 1:Negative, 2:Neutral, 3:Positive, 4:Very Positive
Return ONLY the numeric ID (0-4).
Answer:"""
        elif dataset_name == "TweetEval":
            return f"""Task: Classify tweet sentiment.
Examples:
Text: "Can't wait for the concert tonight! #excited" -> Class: 2
Text: "My flight got cancelled again. Ugh." -> Class: 0
Text: "Just had lunch." -> Class: 1

Classify this:
Text: "{text}"
Options: 0:Negative, 1:Neutral, 2:Positive
Return ONLY the numeric ID (0-2).
Answer:"""

    # ------------------- Zero-Shot Prompts (Model Specific) -------------------
    elif shot_mode == "zero-shot":
        # Qwen Zero-Shot (SMP2020 ä½¿ç”¨ä¸­æ–‡æŒ‡ä»¤)
        if "Qwen" in model_short_name:
            if dataset_name == "SMP2020":
                return f"""åˆ†æè¿™å¥è¯çš„æƒ…æ„Ÿã€‚
æ–‡æœ¬: "{text}"
é€‰é¡¹:
0: æ„¤æ€’
1: ææƒ§
2: é«˜å…´
3: ä¸­æ€§
4: æ‚²ä¼¤
5: æƒŠå¥‡
è¯·åªå›ç­”ä¸€ä¸ªæ•°å­— ID (0-5)ã€‚ä¸è¦è§£é‡Šã€‚
ç­”æ¡ˆ:"""
            elif dataset_name == "SST-5":
                return f"""Classify the sentiment.
Text: "{text}"
Options:
0: Very Negative
1: Negative
2: Neutral
3: Positive
4: Very Positive
Return ONLY the numeric ID (0-4). Do not explain.
Answer:"""
            elif dataset_name == "TweetEval":
                return f"""Classify the sentiment.
Text: "{text}"
Options:
0: Negative
1: Neutral
2: Positive
Return ONLY the numeric ID (0-2). Do not explain.
Answer:"""
        
        # Llama Zero-Shot (SMP2020 ä½¿ç”¨è‹±æ–‡æŒ‡ä»¤ä»¥æé«˜ç¨³å®šæ€§)
        elif "Llama" in model_short_name:
            if dataset_name == "SMP2020":
                return f"""Analyze the sentiment of the following Chinese text.
Text: "{text}"
Options:
0: æ„¤æ€’ (Angry)
1: ææƒ§ (Fear)
2: é«˜å…´ (Happy)
3: ä¸­æ€§ (Neutral)
4: æ‚²ä¼¤ (Sad)
5: æƒŠå¥‡ (Surprise)
Return ONLY the numeric ID (0-5). Do not explain.
Answer:"""
            elif dataset_name == "SST-5":
                return f"""Classify the sentiment of the text.
Text: "{text}"
Options:
0: Very Negative
1: Negative
2: Neutral
3: Positive
4: Very Positive
Return ONLY the numeric ID (0-4). Do not explain.
Answer:"""
            elif dataset_name == "TweetEval":
                return f"""Classify the sentiment of the tweet.
Text: "{text}"
Options:
0: Negative
1: Neutral
2: Positive
Return ONLY the numeric ID (0-2). Do not explain.
Answer:"""
    
    return ""

def parse_prediction(response, dataset_name, model_short_name):
    """
    ç»Ÿä¸€è§£æé€»è¾‘
    """
    try:
        # æ­£åˆ™æå–ç¬¬ä¸€ä¸ªæ•°å­—
        match = re.search(r'\d', response)
        if match:
            return int(match.group())
        else:
            # å…œåº•ç­–ç•¥ (æ ¹æ®æ•°æ®é›†åˆ†å¸ƒçŒœæµ‹å¤§ç±»)
            if dataset_name == "SMP2020": return 3 # ä¸­æ€§/Majority
            if dataset_name == "SST-5": return 2   # Neutral
            return 1 # TweetEval Neutral
    except:
        return 1

# =========================== ğŸš€ æ ¸å¿ƒæ¨ç†å¾ªç¯ ===========================

def run_inference_for_model(model_path, model_short_name):
    """
    åŠ è½½ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶è¿è¡Œå®ƒæ‰€æœ‰çš„ Zero-shot å’Œ 3-shot ä»»åŠ¡
    """
    print(f"\n\n{'='*20} ğŸ¤– Loading Model: {model_short_name} {'='*20}")
    
    # 1. 4-bit é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    # 2. åŠ è½½ Tokenizer & Model
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token_id = tokenizer.eos_token_id
            
    except Exception as e:
        print(f"âŒ Failed to load {model_short_name}: {e}")
        return []

    model_results = []
    
    # 3. éå†ä¸¤ç§æ¨¡å¼: Zero-shot å’Œ 3-shot
    modes = ["zero-shot", "3-shot"]
    
    for mode in modes:
        print(f"\nâ¡ï¸  Mode: {mode.upper()}")
        
        # System Prompt è®¾ç½®
        if mode == "3-shot":
            sys_msg = "You are a helpful sentiment analysis assistant. Follow the examples provided."
        else:
            sys_msg = "You are a helpful sentiment analysis assistant. You output only numeric class IDs."

        for ds_name in DATASETS:
            val_df = get_validation_set(ds_name)
            preds = []
            labels = val_df['label'].tolist()
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            iterator = tqdm(val_df['text'], desc=f"   Running {ds_name}", leave=False)
            
            for text in iterator:
                # è·å– Prompt
                content = get_prompt_content(ds_name, text, model_short_name, mode)
                
                messages = [
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": content}
                ]
                
                # Apply Chat Template
                text_input = tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
                
                inputs = tokenizer([text_input], return_tensors="pt").to(model.device)
                
                with torch.no_grad():
                    outputs = model.generate(
                        inputs.input_ids,
                        max_new_tokens=5,     # é™åˆ¶è¾“å‡ºé•¿åº¦
                        do_sample=False,      # è´ªå©ªè§£ç 
                        temperature=0.0,
                        pad_token_id=tokenizer.pad_token_id
                    )
                
                # Decode Response
                response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()
                
                # Parse
                pred = parse_prediction(response, ds_name, model_short_name)
                preds.append(pred)
            
            # Calculate Metrics
            macro_f1 = f1_score(labels, preds, average="macro")
            acc = accuracy_score(labels, preds)
            
            print(f"   âœ… {ds_name}: Macro-F1 = {macro_f1:.4f}, Acc = {acc:.4f}")
            
            model_results.append({
                "Model": model_short_name,
                "Mode": mode,
                "Dataset": ds_name,
                "Macro-F1": macro_f1,
                "Accuracy": acc
            })

    # 4. æ¸…ç†æ˜¾å­˜ (Crucial for running multiple models in one script)
    print(f"ğŸ—‘ï¸  Unloading {model_short_name}...")
    del model
    del tokenizer
    gc.collect()
    torch.cuda.empty_cache()
    
    return model_results

# =========================== ğŸ ä¸»ç¨‹åºå…¥å£ ===========================

if __name__ == "__main__":
    all_results = []
    
    # ä¾æ¬¡è¿è¡Œæ¯ä¸ªæ¨¡å‹
    for model_path, model_short_name in MODELS_TO_TEST:
        results = run_inference_for_model(model_path, model_short_name)
        all_results.extend(results)
    
    # è¾“å‡ºå¹¶ä¿å­˜æœ€ç»ˆè¡¨æ ¼
    final_df = pd.DataFrame(all_results)
    
    print("\n\n" + "="*50)
    print("ğŸ† FINAL BENCHMARK RESULTS SUMMARY")
    print("="*50)
    print(final_df.to_string(index=False))
    
    # ä¿å­˜ä¸º CSV
    final_df.to_csv("exp_llm_comparison_results.csv", index=False)
    print("\nğŸ“„ Results saved to 'exp_llm_comparison_results.csv'")